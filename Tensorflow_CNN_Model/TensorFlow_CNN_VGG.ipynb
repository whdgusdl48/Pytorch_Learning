{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intel Image Classification (CNN - Keras)\n\nHello, I hope you are having a great day.\n\nIn this notebook, I will try the process of implementing CNN with Keras in order to classify images.\n1. Firstly, we'll import usefull packages.\n1. Then, we'll load the data, before visualize and preprocess it.\n1. We'll try a simple CNN model and then we will evaluate its performances.\n1. We will then use pre trained model to address this challenge aswell."},{"metadata":{},"cell_type":"markdown","source":"# Import Packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn; sn.set(font_scale=1.4)\nfrom sklearn.utils import shuffle           \nimport matplotlib.pyplot as plt             \nimport cv2                                 \nimport tensorflow as tf                \nfrom tqdm import tqdm","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['mountain', 'street', 'glacier', 'buildings', 'sea', 'forest']\nclass_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n\nnb_classes = len(class_names)\n\nIMAGE_SIZE = (224, 224)","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the Data\nWe have to write a load_data function that load the images and the labels from the folder."},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data():\n    \"\"\"\n        Load the data:\n            - 14,034 images to train the network.\n            - 3,000 images to evaluate how accurately the network learned to classify images.\n    \"\"\"\n    \n    datasets = ['../input/seg_train/seg_train', '../input/seg_test/seg_test']\n    output = []\n    \n    # Iterate through training and test sets\n    for dataset in datasets:\n        \n        images = []\n        labels = []\n        \n        print(\"Loading {}\".format(dataset))\n        \n        # Iterate through each folder corresponding to a category\n        for folder in os.listdir(dataset):\n            label = class_names_label[folder]\n            \n            # Iterate through each image in our folder\n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n                \n                # Get the path name of the image\n                img_path = os.path.join(os.path.join(dataset, folder), file)\n                \n                # Open and resize the img\n                image = cv2.imread(img_path)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                image = cv2.resize(image, IMAGE_SIZE) \n                \n                # Append the image and its corresponding label to the output\n                images.append(image)\n                labels.append(label)\n                \n        images = np.array(images, dtype = 'float32')\n        labels = np.array(labels, dtype = 'int32')   \n        \n        output.append((images, labels))\n\n    return output","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train_images, train_labels), (test_images, test_labels) = load_data()","execution_count":5,"outputs":[{"output_type":"stream","text":"  1%|          | 26/2404 [00:00<00:09, 259.92it/s]","name":"stderr"},{"output_type":"stream","text":"Loading ../input/seg_train/seg_train\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 2404/2404 [00:07<00:00, 332.73it/s]\n100%|██████████| 2274/2274 [00:05<00:00, 391.56it/s]\n100%|██████████| 2271/2271 [00:06<00:00, 350.59it/s]\n100%|██████████| 2382/2382 [00:06<00:00, 351.95it/s]\n100%|██████████| 2512/2512 [00:06<00:00, 401.32it/s]\n100%|██████████| 2191/2191 [00:06<00:00, 361.24it/s]\n  6%|▌         | 34/553 [00:00<00:01, 337.44it/s]","name":"stderr"},{"output_type":"stream","text":"Loading ../input/seg_test/seg_test\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 553/553 [00:01<00:00, 396.78it/s]\n100%|██████████| 510/510 [00:01<00:00, 412.24it/s]\n100%|██████████| 474/474 [00:01<00:00, 311.90it/s]\n100%|██████████| 501/501 [00:01<00:00, 347.41it/s]\n100%|██████████| 525/525 [00:01<00:00, 392.83it/s]\n100%|██████████| 437/437 [00:01<00:00, 391.70it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images, train_labels = shuffle(train_images, train_labels, random_state=25)","execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'shuffle' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-a2bccc3c64d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'shuffle' is not defined"]}]},{"metadata":{},"cell_type":"markdown","source":"# Let's explore the dataset\nWe can ask ourselves:\n* How many training and testing examples do we have ?\n* What is the size of the images ?\n* What is the proportion of each observed category ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_train = train_labels.shape[0]\nn_test = test_labels.shape[0]\n\nprint (\"Number of training examples: {}\".format(n_train))\nprint (\"Number of testing examples: {}\".format(n_test))\nprint (\"Each image is of size: {}\".format(IMAGE_SIZE))","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_labels' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6fc62d7a134b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of training examples: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of testing examples: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\n_, train_counts = np.unique(train_labels, return_counts=True)\n_, test_counts = np.unique(test_labels, return_counts=True)\npd.DataFrame({'train': train_counts,\n                    'test': test_counts}, \n             index=class_names\n            ).plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.pie(train_counts,\n        explode=(0, 0, 0, 0, 0, 0) , \n        labels=class_names,\n        autopct='%1.1f%%')\nplt.axis('equal')\nplt.title('Proportion of each observed category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Good practice: scale the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = train_images / 255.0 \ntest_images = test_images / 255.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize the data\nWe can display a random image from the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_random_image(class_names, images, labels):\n    \"\"\"\n        Display a random image from the images array and its correspond label from the labels array.\n    \"\"\"\n    \n    index = np.random.randint(images.shape[0])\n    plt.figure()\n    plt.imshow(images[index])\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.title('Image #{} : '.format(index) + class_names[labels[index]])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_random_image(class_names, train_images, train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also display the first 25 images from the training set directly with a loop to get a better view"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_examples(class_names, images, labels):\n    \"\"\"\n        Display 25 images from the images array with its corresponding labels\n    \"\"\"\n    \n    fig = plt.figure(figsize=(10,10))\n    fig.suptitle(\"Some examples of images of the dataset\", fontsize=16)\n    for i in range(25):\n        plt.subplot(5,5,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(images[i], cmap=plt.cm.binary)\n        plt.xlabel(class_names[labels[i]])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_examples(class_names, train_images, train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Beginner: Simple Model Creation\n\nSteps are:\n1. Build the model,\n1. Compile the model,\n1. Train / fit the data to the model,\n1. Evaluate the model on the testing set,\n1. Carry out an error analysis of our model.\n\nWe can build an easy model composed of different layers such as:\n* Conv2D: (32 filters of size 3 by 3) The features will be \"extracted\" from the image.\n* MaxPooling2D: The images get half sized.\n* Flatten: Transforms the format of the images from a 2d-array to a 1d-array of 150 150 3 pixel values.\n* Relu  : given a value x, returns max(x, 0).\n* Softmax: 6 neurons, probability that the image belongs to one of the classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_VGGnet_A = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (224, 224, 3)), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_VGGnet_A-LRN = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (224, 224, 3)), \n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_VGGnet_B = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (224, 224, 3)), \n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_VGGnet_C = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (224, 224, 3)), \n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (1, 1), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (1, 1), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (1, 1), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_VGG16 = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (224, 224, 3)), \n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_VGG19 = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', input_shape = (224, 224, 3)), \n    tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(256, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.Conv2D(512, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(4096, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we can compile it with some parameters such as:\n* **Optimizer**: adam = RMSProp + Momentum.\nWhat is Momentum and RMSProp ?\n* Momentum = takes into account past gradient to have a better update.\n* RMSProp = exponentially weighted average of the squares of past gradients.\n* **Loss function**: we use sparse categorical crossentropy for classification, each images belongs to one class only"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fit the model to the data from the training set. The neural network will learn by itself the pattern in order to distinguish each category."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_images, train_labels, batch_size=128, epochs=20, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_accuracy_loss(history):\n    \"\"\"\n        Plot the accuracy and the loss during the training of the nn.\n    \"\"\"\n    fig = plt.figure(figsize=(10,5))\n\n    # Plot accuracy\n    plt.subplot(221)\n    plt.plot(history.history['acc'],'bo--', label = \"acc\")\n    plt.plot(history.history['val_acc'], 'ro--', label = \"val_acc\")\n    plt.title(\"train_acc vs val_acc\")\n    plt.ylabel(\"accuracy\")\n    plt.xlabel(\"epochs\")\n    plt.legend()\n\n    # Plot loss function\n    plt.subplot(222)\n    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n    plt.title(\"train_loss vs val_loss\")\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"epochs\")\n\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_accuracy_loss(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should evaluate the model performance on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss = model.evaluate(test_images, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we achieve 0.76 accuracy on the testing test. We got a slight underfitting :(\n\nLet's see how the classifier is doing on random images."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_images)     # Vector of probabilities\npred_labels = np.argmax(predictions, axis = 1) # We take the highest probability\n\ndisplay_random_image(class_names, test_images, pred_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Error analysis\n\nWe can try to understand on which kind of images the classifier has trouble."},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_mislabeled_images(class_names, test_images, test_labels, pred_labels):\n    \"\"\"\n        Print 25 examples of mislabeled images by the classifier, e.g when test_labels != pred_labels\n    \"\"\"\n    BOO = (test_labels == pred_labels)\n    mislabeled_indices = np.where(BOO == 0)\n    mislabeled_images = test_images[mislabeled_indices]\n    mislabeled_labels = pred_labels[mislabeled_indices]\n\n    title = \"Some examples of mislabeled images by the classifier:\"\n    display_examples(class_names,  mislabeled_images, mislabeled_labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_mislabeled_images(class_names, test_images, test_labels, pred_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CM = confusion_matrix(test_labels, pred_labels)\nax = plt.axes()\nsn.heatmap(CM, annot=True, \n           annot_kws={\"size\": 10}, \n           xticklabels=class_names, \n           yticklabels=class_names, ax = ax)\nax.set_title('Confusion matrix')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion: The classifier has trouble with 2 kinds of images.\nIt has trouble with street and buildings. Well, it can be understandable as as there are buildings in the street. \nIt has also trouble with sea, glacier and moutain as well. It is hard for me to fully distinguish them.\nHowever, it can detects forest very accurately!"},{"metadata":{},"cell_type":"markdown","source":"**Intermediate Update January 2020**\n\n* Feature extraction with VGG16 trained on ImageNet\n\n\n* Ensemble models of Neural Networks with the features extracted from VGG\n\nInspired from: https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/\n\n* Fine Tuning with VGG16 trained on ImageNet"},{"metadata":{},"cell_type":"markdown","source":"# Feature extraction with VGG ImageNet"},{"metadata":{},"cell_type":"markdown","source":"We can extract features from VGG16."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\n\nmodel = VGG16(weights='imagenet', include_top=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get the features directly from VGG16"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = model.predict(train_images)\ntest_features = model.predict(test_images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize the features through PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_train, x, y, z = train_features.shape\nn_test, x, y, z = test_features.shape\nnumFeatures = x * y * z","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import decomposition\n\npca = decomposition.PCA(n_components = 2)\n\nX = train_features.reshape((n_train, x*y*z))\npca.fit(X)\n\nC = pca.transform(X) # Représentation des individus dans les nouveaux axe\nC1 = C[:,0]\nC2 = C[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Figures\n\nplt.subplots(figsize=(10,10))\n\nfor i, class_name in enumerate(class_names):\n    plt.scatter(C1[train_labels == i][:1000], C2[train_labels == i][:1000], label = class_name, alpha=0.4)\nplt.legend()\nplt.title(\"PCA Projection\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can identifying clusters thanks to this PCA. The clusters correspond more or less to the labels.\n\nWe see that glacier and mountain points are very close to each other, as VGG sees them as very similar.\n\nWe see that there is no distinction between building and street.\n "},{"metadata":{},"cell_type":"markdown","source":"## Training on top of VGG\n\nLet's train a simple one-layer Neural Network on the features extracted from VGG."},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape = (x, y, z)),\n    tf.keras.layers.Dense(50, activation=tf.nn.relu),\n    tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n])\n\nmodel2.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory2 = model2.fit(train_features, train_labels, batch_size=128, epochs=15, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_accuracy_loss(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should get approximately 0.844 accuracy (+0.1 accuracy) over the simple ConvNet."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss = model2.evaluate(test_features, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Neural Networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(seed=1997)\n# Number of estimators\nn_estimators = 10\n# Proporition of samples to use to train each training\nmax_samples = 0.8\n\nmax_samples *= n_train\nmax_samples = int(max_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define n_estimators Neural Networks. \n\nEach Neural Network will be trained on random subsets of the training dataset. Each subset contains max_samples samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = list()\nrandom = np.random.randint(50, 100, size = n_estimators)\n\nfor i in range(n_estimators):\n    \n    # Model\n    model = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape = (x, y, z)),\n                                # One layer with random size\n                                    tf.keras.layers.Dense(random[i], activation=tf.nn.relu),\n                                    tf.keras.layers.Dense(6, activation=tf.nn.softmax)\n                                ])\n    \n    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    # Store model\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"histories = []\n\nfor i in range(n_estimators):\n    # Train each model on a bag of the training data\n    train_idx = np.random.choice(len(train_features), size = max_samples)\n    histories.append(models[i].fit(train_features[train_idx], train_labels[train_idx], batch_size=128, epochs=10, validation_split = 0.1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We aggregate each model individual predictions to form a final prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor i in range(n_estimators):\n    predictions.append(models[i].predict(test_features))\n    \npredictions = np.array(predictions)\npredictions = predictions.sum(axis = 0)\npred_labels = predictions.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should improve our result as we have a lower variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy : {}\".format(accuracy_score(test_labels, pred_labels)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fine Tuning VGG ImageNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\n\nmodel = VGG16(weights='imagenet', include_top=False)\nmodel = Model(inputs=model.inputs, outputs=model.layers[-5].output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = model.predict(train_images)\ntest_features = model.predict(test_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input, Dense, Conv2D, Activation , MaxPooling2D, Flatten\n\nmodel2 = VGG16(weights='imagenet', include_top=False)\n\ninput_shape = model2.layers[-4].get_input_shape_at(0) # get the input shape of desired layer\nlayer_input = Input(shape = (9, 9, 512)) # a new input tensor to be able to feed the desired layer\n# https://stackoverflow.com/questions/52800025/keras-give-input-to-intermediate-layer-and-get-final-output\n\nx = layer_input\nfor layer in model2.layers[-4::1]:\n    x = layer(x)\n    \nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Flatten()(x)\nx = Dense(100,activation='relu')(x)\nx = Dense(6,activation='softmax')(x)\n\n# create the model\nnew_model = Model(layer_input, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = new_model.fit(train_features, train_labels, batch_size=128, epochs=10, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_accuracy_loss(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\npredictions = new_model.predict(test_features)    \npred_labels = np.argmax(predictions, axis = 1)\nprint(\"Accuracy : {}\".format(accuracy_score(test_labels, pred_labels)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}